# IndestructibleEco v1.0 — Helm Values
# URI: indestructibleeco://helm/values
global:
  namespace: indestructibleeco
  imageRegistry: "ghcr.io/indestructibleorg"
  imagePullSecrets: []

# ── API Gateway ───────────────────────────────────────────────────────
api:
  replicaCount: 3
  image:
    repository: ghcr.io/indestructibleorg/api
    tag: "1.0.0"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 8000
  resources:
    requests:
      cpu: "500m"
      memory: "1Gi"
    limits:
      cpu: "2000m"
      memory: "4Gi"
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilization: 70
  env:
    ECO_ENVIRONMENT: production
    ECO_LOG_LEVEL: INFO
    ECO_WORKERS: "4"

# ── AI Service ────────────────────────────────────────────────────────
ai:
  replicaCount: 2
  image:
    repository: ghcr.io/indestructibleorg/ai
    tag: "1.0.0"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    httpPort: 8001
    grpcPort: 8000
  resources:
    requests:
      cpu: "500m"
      memory: "1Gi"
    limits:
      cpu: "2000m"
      memory: "4Gi"
      nvidia.com/gpu: "1"
  env:
    ECO_ENVIRONMENT: production
    ECO_AI_MODELS: "vllm,ollama,tgi,sglang"
    ECO_VECTOR_DIM: "1024"
    ECO_ALIGNMENT_MODEL: "quantum-bert-xxl-v1"

# ── vLLM Engine ───────────────────────────────────────────────────────
vllm:
  enabled: true
  replicas: 1
  image:
    repository: vllm/vllm-openai
    tag: "v0.6.6"
  model: "meta-llama/Llama-3.1-8B-Instruct"
  gpuMemoryUtilization: "0.90"
  maxModelLen: "32768"
  tensorParallelSize: "1"
  enablePrefixCaching: true
  port: 8001
  resources:
    requests:
      cpu: "4"
      memory: "16Gi"
      nvidia.com/gpu: "1"
    limits:
      cpu: "8"
      memory: "32Gi"
      nvidia.com/gpu: "1"
  storage:
    size: 100Gi
    storageClass: fast-ssd

# ── SGLang Engine ─────────────────────────────────────────────────────
sglang:
  enabled: true
  replicas: 1
  image:
    repository: lmsysorg/sglang
    tag: "v0.3.6-cu124"
  model: "meta-llama/Llama-3.1-8B-Instruct"
  memFractionStatic: "0.88"
  port: 8003
  resources:
    requests:
      cpu: "4"
      memory: "16Gi"
      nvidia.com/gpu: "1"
    limits:
      cpu: "8"
      memory: "32Gi"
      nvidia.com/gpu: "1"
  storage:
    size: 100Gi
    storageClass: fast-ssd

# ── TGI Engine ────────────────────────────────────────────────────────
tgi:
  enabled: true
  replicas: 1
  image:
    repository: ghcr.io/huggingface/text-generation-inference
    tag: "2.4.1"
  model: "meta-llama/Llama-3.1-8B-Instruct"
  maxInputLength: 4096
  maxTotalTokens: 8192
  port: 8002
  resources:
    requests:
      cpu: "4"
      memory: "16Gi"
      nvidia.com/gpu: "1"
    limits:
      cpu: "8"
      memory: "32Gi"
      nvidia.com/gpu: "1"
  storage:
    size: 100Gi
    storageClass: fast-ssd

# ── Ollama Engine ─────────────────────────────────────────────────────
ollama:
  enabled: false
  replicas: 1
  image:
    repository: ollama/ollama
    tag: "latest"
  port: 11434
  resources:
    requests:
      cpu: "2"
      memory: "8Gi"
      nvidia.com/gpu: "1"
    limits:
      cpu: "4"
      memory: "16Gi"
      nvidia.com/gpu: "1"
  storage:
    size: 100Gi
    storageClass: fast-ssd

# ── Redis ─────────────────────────────────────────────────────────────
redis:
  enabled: true
  image:
    repository: redis
    tag: "7-alpine"
  resources:
    requests:
      cpu: "250m"
      memory: "512Mi"
    limits:
      cpu: "1000m"
      memory: "3Gi"
  storage:
    size: 10Gi

# ── PostgreSQL ────────────────────────────────────────────────────────
postgres:
  enabled: true
  image:
    repository: postgres
    tag: "16-alpine"
  database: indestructibleeco
  user: indestructibleeco
  resources:
    requests:
      cpu: "250m"
      memory: "512Mi"
    limits:
      cpu: "2000m"
      memory: "4Gi"
  storage:
    size: 50Gi

# ── Monitoring ────────────────────────────────────────────────────────
monitoring:
  prometheus:
    enabled: true
    retention: 30d
    storage:
      size: 50Gi
  grafana:
    enabled: true
    adminPassword: ""  # Set via secret
    storage:
      size: 5Gi

# ── Ingress ───────────────────────────────────────────────────────────
ingress:
  enabled: true
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
    - host: api.indestructibleeco.io
      paths:
        - path: /
          pathType: Prefix
          service: eco-api-svc
          port: 8000
    - host: ai.indestructibleeco.io
      paths:
        - path: /
          pathType: Prefix
          service: eco-ai-svc
          port: 8001
  tls:
    - secretName: eco-tls
      hosts:
        - api.indestructibleeco.io
        - ai.indestructibleeco.io

# ── Secrets (override via --set or external secret manager) ──────────
secrets:
  secretKey: ""
  jwtSecret: ""
  postgresPassword: ""
  hfToken: ""
# -- Top-level defaults for Helm templates (deployment.yaml, hpa.yaml, pdb.yaml)
namespace: indestructibleeco
replicaCount: 2

image:
  repository: ghcr.io/indestructibleorg/ai
  tag: "1.0.0"
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 8001
  grpcPort: 8000

resources:
  requests:
    cpu: "500m"
    memory: "512Mi"
  limits:
    cpu: "2000m"
    memory: "4Gi"

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

podDisruptionBudget:
  enabled: true
  maxUnavailable: 1

env: {}
nodeSelector: {}
tolerations: []
affinity: {}
