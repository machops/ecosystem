# eco-base v1.0 - TGI Inference Engine StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: tgi
  namespace: eco-base
  labels:
    app.kubernetes.io/name: tgi
    app.kubernetes.io/component: inference-engine
    app.kubernetes.io/part-of: eco-base
spec:
  serviceName: tgi-svc
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tgi
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tgi
        app.kubernetes.io/component: inference-engine
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8002"
    spec:
      serviceAccountName: eco-sa
      terminationGracePeriodSeconds: 120
      containers:
        - name: tgi
          image: ghcr.io/huggingface/text-generation-inference:2.4.1
          ports:
            - name: http
              containerPort: 8002
          args:
            - "--model-id=meta-llama/Llama-3.1-8B-Instruct"
            - "--port=8002"
            - "--max-input-length=4096"
            - "--max-total-tokens=8192"
            - "--max-batch-prefill-tokens=4096"
            - "--max-concurrent-requests=128"
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: eco-secrets
                  key: HF_TOKEN
          resources:
            requests:
              cpu: "4"
              memory: "16Gi"
              nvidia.com/gpu: "1"
            limits:
              cpu: "8"
              memory: "32Gi"
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: model-cache
              mountPath: /data
            - name: shm
              mountPath: /dev/shm
          readinessProbe:
            httpGet:
              path: /health
              port: 8002
            initialDelaySeconds: 120
            periodSeconds: 15
            timeoutSeconds: 10
            failureThreshold: 10
          livenessProbe:
            httpGet:
              path: /health
              port: 8002
            initialDelaySeconds: 180
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 5
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 8Gi
      nodeSelector:
        nvidia.com/gpu.present: "true"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
  volumeClaimTemplates:
    - metadata:
        name: model-cache
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: fast-ssd
        resources:
          requests:
            storage: 100Gi
---
apiVersion: v1
kind: Service
metadata:
  name: tgi-svc
  namespace: eco-base
  labels:
    app.kubernetes.io/name: tgi
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8002
      targetPort: 8002
  selector:
    app.kubernetes.io/name: tgi
---
# YAML Toolkit v8 â€” Governance Block (auto-generated, manual editing prohibited)
document_metadata:
  unique_id: "eco-tg-0001-0001-000000000001"
  uri: "eco-base://k8s/eco-base/statefulset/tgi"
  urn: "urn:eco-base:k8s:eco-base:statefulset:tgi:eco-tg-0001-0001-000000000001"
  target_system: gke-production
  cross_layer_binding: [api-gateway, ai-service]
  schema_version: v8
  generated_by: yaml-toolkit-v8
  created_at: "2025-02-18T00:00:00.000Z"
governance_info:
  owner: platform-team
  approval_chain: [platform-team, ml-team]
  compliance_tags: [zero-trust, soc2, internal, gpu-workload, inference]
  lifecycle_policy: active
registry_binding:
  service_endpoint: "http://tgi-svc.eco-base.svc.cluster.local:8002"
  discovery_protocol: consul
  health_check_path: "/health"
  registry_ttl: 30
vector_alignment_map:
  alignment_model: BAAI/bge-large-en-v1.5
  coherence_vector_dim: 1024
  function_keyword: [tgi, inference, gpu, text-generation, huggingface]
  contextual_binding: "tgi -> [api-gateway, ai-service]"
