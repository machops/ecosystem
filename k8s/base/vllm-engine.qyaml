# IndestructibleEco v1.0 - vLLM Inference Engine StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: vllm
  namespace: indestructibleeco
  labels:
    app.kubernetes.io/name: vllm
    app.kubernetes.io/component: inference-engine
    app.kubernetes.io/part-of: indestructibleeco
spec:
  serviceName: vllm-svc
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: vllm
  template:
    metadata:
      labels:
        app.kubernetes.io/name: vllm
        app.kubernetes.io/component: inference-engine
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8001"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: eco-sa
      terminationGracePeriodSeconds: 120
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.6.6
          ports:
            - name: http
              containerPort: 8001
          args:
            - "--host=0.0.0.0"
            - "--port=8001"
            - "--model=meta-llama/Llama-3.1-8B-Instruct"
            - "--gpu-memory-utilization=0.90"
            - "--max-model-len=32768"
            - "--enable-prefix-caching"
            - "--enable-chunked-prefill"
            - "--disable-log-requests"
            - "--served-model-name=llama-3.1-8b-instruct"
          env:
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: eco-secrets
                  key: HF_TOKEN
            - name: VLLM_ATTENTION_BACKEND
              value: "FLASH_ATTN"
          resources:
            requests:
              cpu: "4"
              memory: "16Gi"
              nvidia.com/gpu: "1"
            limits:
              cpu: "8"
              memory: "32Gi"
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
            - name: shm
              mountPath: /dev/shm
          readinessProbe:
            httpGet:
              path: /health
              port: 8001
            initialDelaySeconds: 120
            periodSeconds: 15
            timeoutSeconds: 10
            failureThreshold: 10
          livenessProbe:
            httpGet:
              path: /health
              port: 8001
            initialDelaySeconds: 180
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 5
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
      nodeSelector:
        nvidia.com/gpu.present: "true"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
  volumeClaimTemplates:
    - metadata:
        name: model-cache
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: fast-ssd
        resources:
          requests:
            storage: 100Gi
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-svc
  namespace: indestructibleeco
  labels:
    app.kubernetes.io/name: vllm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8001
      targetPort: 8001
  selector:
    app.kubernetes.io/name: vllm
---
# YAML Toolkit v1 â€” Governance Block (auto-generated, manual editing prohibited)
document_metadata:
  unique_id: "eco-vl-0001-0001-000000000001"
  uri: "indestructibleeco://k8s/indestructibleeco/statefulset/vllm"
  urn: "urn:indestructibleeco:k8s:indestructibleeco:statefulset:vllm:eco-vl-0001-0001-000000000001"
  target_system: gke-production
  cross_layer_binding: [api-gateway, ai-service]
  schema_version: v1
  generated_by: yaml-toolkit-v1
  created_at: "2025-02-18T00:00:00.000Z"
governance_info:
  owner: platform-team
  approval_chain: [platform-team, ml-team]
  compliance_tags: [zero-trust, soc2, internal, gpu-workload, inference]
  lifecycle_policy: active
registry_binding:
  service_endpoint: "http://vllm-svc.indestructibleeco.svc.cluster.local:8001"
  discovery_protocol: consul
  health_check_path: "/health"
  registry_ttl: 30
vector_alignment_map:
  alignment_model: quantum-bert-xxl-v1
  coherence_vector_dim: 1024
  function_keyword: [vllm, inference, gpu, text-generation, streaming]
  contextual_binding: "vllm -> [api-gateway, ai-service]"
