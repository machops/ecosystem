# @ECO-governed
# @ECO-layer: GL90-99
# @ECO-semantic: archive-tools
# @ECO-audit-trail: ../../engine/gl-platform.governance/GL_SEMANTIC_ANCHOR.json
#
# GL Unified Architecture Governance Framework Activated
#
# @ECO-governed
# @ECO-layer: gl-platform.governance
# @ECO-semantic: autonomous_cleanup_toolkit
# @ECO-audit-trail: ../../engine/gl-platform.governance/GL_SEMANTIC_ANCHOR.json
#
#!/usr/bin/env python3
# GL Layer: GL30-49 Execution Layer
# Module: Marker Detection (Phase 6.2)
# Semantic Anchor: urn:machinenativeops:gl:tools:autonomous-cleanup:1.0.0
"""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        Autonomous Cleanup Toolkit - Claude Code Capabilities
                     Ëá™‰∏ªÊ∏ÖÁêÜÂ∑•ÂÖ∑ÂåÖ - Claude ‰ª£Á¢ºËÉΩÂäõ
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
GL Layer: GL30-49 Execution Layer
GL Unified Architecture Governance Framework: Activated
Semantic Root: urn:machinenativeops:gl:tools:autonomous-cleanup:1.1.0
This toolkit replicates the cleanup capabilities demonstrated in the
Claude Code session for repository maintenance and technical debt cleanup.
Ê≠§Â∑•ÂÖ∑ÂåÖË§áË£Ω‰∫Ü Claude Code ÊúÉË©±‰∏≠Â±ïÁ§∫ÁöÑÊ∏ÖÁêÜËÉΩÂäõÔºåÁî®ÊñºÂÑ≤Â≠òÂ∫´Á∂≠Ë≠∑ÂíåÊäÄË°ìÂÇµÂãôÊ∏ÖÁêÜ„ÄÇ
Features | ÂäüËÉΩ:
-----------------
1. üîç Duplicate File Detection (SHA256-based)
2. üßπ TODO Marker Analysis and Cleanup
3. ‚ö†Ô∏è  NotImplementedError Detection
4. üìä Technical Debt Scanning
5. üîí P0 Safety Verification
6. ü§ñ Automated Fix Suggestions
7. üìà Progress Tracking and Reporting
8. üîÑ Git Workflow Automation
Usage | ‰ΩøÁî®ÊñπÊ≥ï:
-----------------
    # Run full cleanup analysis
    python autonomous_cleanup_toolkit.py analyze
    # Execute specific cleanup phase
    python autonomous_cleanup_toolkit.py cleanup --phase duplicates
    python autonomous_cleanup_toolkit.py cleanup --phase todos
    # Run P0 safety verification
    python autonomous_cleanup_toolkit.py safety
    # Git workflow automation
    python autonomous_cleanup_toolkit.py git --action status
    # Generate progress report
    python autonomous_cleanup_toolkit.py report
Author: Synthesized from Claude Code Session
Version: 1.1.0
Date: 2026-01-21
Date: 2025-12-16
GL Unified Architecture Governance Framework: Activated
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""
# MNGA-002: Import organization needs review
import argparse
import hashlib
import json
import logging
import re
import subprocess
from collections import defaultdict
from dataclasses import asdict, dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional
# =============================================================================
# Configuration
# =============================================================================
BASE_PATH = Path(__file__).parent.parent
TOOLS_PATH = BASE_PATH / "tools"
# Color codes for terminal output
class Colors:
    GREEN = "\033[92m"
    YELLOW = "\033[93m"
    RED = "\033[91m"
    BLUE = "\033[94m"
    CYAN = "\033[96m"
    END = "\033[0m"
    BOLD = "\033[1m"
# =============================================================================
# Data Models
# =============================================================================
@dataclass
class TodoItem:
    """Represents a detected TODO marker in source code."""
    file_path: str
    line_number: int
    todo_type: str  # TODO, FIXME, XXX, HACK, DEPRECATED
    message: str
    severity: str  # HIGH, MEDIUM, LOW
    context: str  # Surrounding code
@dataclass
class DuplicateGroup:
    """Group of duplicate files"""
    file_hash: str  # SHA256 hash of file content
    files: List[str]
    size_bytes: int
    removable: List[str]  # Files that can be safely removed
@dataclass
class NotImplementedStub:
    """NotImplementedError or stub function"""
    file_path: str
    function_name: str
    line_number: int
    class_name: Optional[str]
@dataclass
class CleanupReport:
    """Overall cleanup progress report"""
    timestamp: str
    phase: str
    items_found: int
    items_fixed: int
    items_remaining: int
    files_modified: int
    lines_added: int
    lines_removed: int
    details: Dict[str, Any] = field(default_factory=dict)
# =============================================================================
# Core Cleanup Engine
# =============================================================================
class AutonomousCleanupEngine:
    """
    Main engine for autonomous repository cleanup.
    Replicates Claude Code's cleanup workflow.
    """
    def __init__(self, repo_path: Path):
        self.repo_path = repo_path
        self.logger = self._setup_logging()
        # Statistics
        self.stats = {
            "scans_performed": 0,
            "items_found": 0,
            "items_fixed": 0,
            "files_modified": 0,
        }
    def _setup_logging(self) -> logging.Logger:
        """Setup logging configuration"""
        log_dir = self.repo_path / ".automation_logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        logger = logging.getLogger("autonomous_cleanup")
        logger.setLevel(logging.INFO)
        # File handler
        fh = logging.FileHandler(log_dir / "autonomous_cleanup.log")
        fh.setLevel(logging.INFO)
        # Console handler
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)
        # Formatter
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        fh.setFormatter(formatter)
        ch.setFormatter(formatter)
        logger.addHandler(fh)
        logger.addHandler(ch)
        return logger
    # =========================================================================
    # Duplicate Detection (Phase 2)
    # =========================================================================
    def find_duplicates(
        self, extensions: List[str] = [".py", ".sh", ".js", ".ts"]
    ) -> List[DuplicateGroup]:
        """
        Find duplicate files using MD5 hashing.
        Ë§áË£ΩËá™ tools/find_duplicate_scripts.py ÁöÑÂäüËÉΩ
        """
        self.logger.info("üîç Scanning for duplicate files...")
        hash_map = defaultdict(list)
        excluded_dirs = {
            ".git",
            "node_modules",
            "__pycache__",
            ".venv",
            "venv",
            "dist",
            "build",
            ".pytest_cache",
        }
        # Scan files
        for ext in extensions:
            for file_path in self.repo_path.rglob(f"*{ext}"):
                # Skip excluded directories
                if any(excluded in file_path.parts for excluded in excluded_dirs):
                    continue
                try:
                    content = file_path.read_bytes()
                    file_hash = hashlib.sha256(content).hexdigest()
                    hash_map[file_hash].append(
                        str(file_path.relative_to(self.repo_path))
                    )
                except Exception as e:
                    self.logger.warning(f"Error reading {file_path}: {e}")
        # Find duplicates
        duplicate_groups = []
        for file_hash, files in hash_map.items():
            if len(files) > 1:
                # Determine which files are removable
                removable = self._identify_removable_duplicates(files)
                # Get file size
                size_bytes = (self.repo_path / files[0]).stat().st_size
                duplicate_groups.append(
                    DuplicateGroup(
                        file_hash=file_hash,
                        files=files,
                        size_bytes=size_bytes,
                        removable=removable,
                    )
                )
        self.logger.info(f"‚úÖ Found {len(duplicate_groups)} groups of duplicates")
        return duplicate_groups
    def _identify_removable_duplicates(self, files: List[str]) -> List[str]:
        """Identify which duplicates can be safely removed"""
        removable = []
        for file in files:
            # Rule 1: Prefer non-legacy versions
            if file.startswith("legacy/"):
                removable.append(file)
            # Rule 2: Prefer services/agents/ over agent/
            elif file.startswith("agent/") and any(
                f.startswith("services/agents/") for f in files
            ):
                removable.append(file)
            # Rule 3: Prefer non-backup versions
            elif ".backup" in file or "_backup" in file:
                removable.append(file)
        return removable
    # =========================================================================
    # TODO Marker Detection (Phase 6.2)
    # =========================================================================
    def find_todos(self) -> List[TodoItem]:
        """
        Find all TODO markers in Python files.
        Ë§áË£ΩËá™ tools/scan_tech_debt.py ÁöÑÂäüËÉΩ
        """
        self.logger.info("üìù Scanning for TODO markers...")
        patterns = {
            "TODO": re.compile(r"#\s*TODO\s*:?\s*(.+)", re.IGNORECASE),
            "FIXME": re.compile(r"#\s*FIXME\s*:?\s*(.+)", re.IGNORECASE),
            "XXX": re.compile(r"#\s*XXX\s*:?\s*(.+)", re.IGNORECASE),
            "HACK": re.compile(r"#\s*HACK\s*:?\s*(.+)", re.IGNORECASE),
            "DEPRECATED": re.compile(r"@deprecated|#\s*DEPRECATED", re.IGNORECASE),
        }
        todos = []
        for py_file in self.repo_path.rglob("*.py"):
            if any(
                excluded in str(py_file)
                for excluded in [".venv", "__pycache__", "node_modules"]
            ):
                continue
            try:
                with open(py_file, "r", encoding="utf-8") as f:
                    lines = f.readlines()
                for line_num, line in enumerate(lines, 1):
                    for todo_type, pattern in patterns.items():
                        match = pattern.search(line)
                        if match:
                            message = (
                                match.group(1)
                                if match.lastindex and match.lastindex >= 1
                                else line.strip()
                            )
                            # Get context (3 lines before and after)
                            context_start = max(0, line_num - 4)
                            context_end = min(len(lines), line_num + 3)
                            context = "".join(lines[context_start:context_end])
                            # Determine severity
                            severity = self._determine_todo_severity(todo_type, message)
                            todos.append(
                                TodoItem(
                                    file_path=str(py_file.relative_to(self.repo_path)),
                                    line_number=line_num,
                                    todo_type=todo_type,
                                    message=message,
                                    severity=severity,
                                    context=context,
                                )
                            )
            except Exception as e:
                self.logger.warning(f"Error scanning {py_file}: {e}")
        self.logger.info(f"‚úÖ Found {len(todos)} TODO markers")
        return todos
    def _determine_todo_severity(self, todo_type: str, message: str) -> str:
        """
        Determine TODO severity based on type and message.
        Aligned with tools/scan_tech_debt.py severity determination logic
        for consistency across the GL gl-platform.governance layer.
        Severity determination order:
            1. Check message for high priority keywords -> HIGH
            2. Check message for medium priority keywords -> MEDIUM
            3. Check todo_type for FIXME/XXX -> MEDIUM
            4. Check todo_type for HACK/DEPRECATED -> HIGH (enhancement)
            5. Default -> LOW
        Note: HACK and DEPRECATED types are handled as HIGH priority as an
        enhancement over scan_tech_debt.py, which does not explicitly handle
        these types. This supports better technical debt tracking.
        Severity Levels:
            HIGH: Critical issues requiring immediate attention
            MEDIUM: Important items that should be addressed
            LOW: Minor items for future consideration
        """
        message_lower = message.lower()
        # Step 1: Check for high priority keywords in message
        # (aligned with scan_tech_debt.py)
        high_priority_keywords = [
            "security",
            "critical",
            "urgent",
            "bug",
            "broken",
            "fix immediately",
        ]
        if any(keyword in message_lower for keyword in high_priority_keywords):
            return "HIGH"
        # Step 2: Check for medium priority keywords in message
        # (aligned with scan_tech_debt.py)
        medium_priority_keywords = [
            "important",
            "should",
            "refactor",
            "improve",
        ]
        if any(keyword in message_lower for keyword in medium_priority_keywords):
            return "MEDIUM"
        # Step 3: FIXME and XXX types default to MEDIUM severity
        # (aligned with scan_tech_debt.py behavior)
        if todo_type in ["FIXME", "XXX"]:
            return "MEDIUM"
        # Step 4: HACK and DEPRECATED return HIGH due to technical debt implications
        # Enhancement: scan_tech_debt.py does not explicitly handle these types
        if todo_type in ["HACK", "DEPRECATED"]:
            return "HIGH"
        # Step 5: Default to LOW for TODO and other types
        return "LOW"
    # =========================================================================
    # NotImplementedError Detection (Phase 4)
    # =========================================================================
    def find_not_implemented_stubs(self) -> List[NotImplementedStub]:
        """Find functions with NotImplementedError"""
        self.logger.info("üöß Scanning for NotImplementedError stubs...")
        stubs = []
        for py_file in self.repo_path.rglob("*.py"):
            if any(excluded in str(py_file) for excluded in [".venv", "__pycache__"]):
                continue
            try:
                with open(py_file, "r", encoding="utf-8") as f:
                    content = f.read()
                # Find NotImplementedError raises
                pattern = re.compile(
                    r"def\s+(\w+)\([^)]*\)[^:]*:\s*"
                    r'(?:"""[^"]*"""\s*)?'
                    r"raise\s+NotImplementedError",
                    re.MULTILINE,
                )
                for match in pattern.finditer(content):
                    function_name = match.group(1)
                    line_number = content[: match.start()].count("\n") + 1
                    # Try to find class name
                    class_pattern = re.compile(r"class\s+(\w+)", re.MULTILINE)
                    class_matches = list(
                        class_pattern.finditer(content[: match.start()])
                    )
                    class_name = class_matches[-1].group(1) if class_matches else None
                    stubs.append(
                        NotImplementedStub(
                            file_path=str(py_file.relative_to(self.repo_path)),
                            function_name=function_name,
                            line_number=line_number,
                            class_name=class_name,
                        )
                    )
            except Exception as e:
                self.logger.warning(f"Error scanning {py_file}: {e}")
        self.logger.info(f"‚úÖ Found {len(stubs)} NotImplementedError stubs")
        return stubs
    # =========================================================================
    # P0 Safety Verification (Phase 5)
    # =========================================================================
    def verify_p0_safety(self) -> Dict[str, Any]:
        """
        Verify P0 critical safety items.
        Integrates with tools/verify_p0_safety.py functionality.
        """
        self.logger.info("üîí Running P0 safety verification...")
        results = {
            "timestamp": datetime.now().isoformat(),
            "checks": [],
            "passed": 0,
            "failed": 0,
            "warnings": 0,
        }
        # Check 1: Emergency stop mechanism
        emergency_files = [
            self.repo_path / "src/core/safety/emergency_stop.py",
            self.repo_path
            / "src/services/agents/dependency-manager/src/crossplatform/emergency_response.py",
        ]
        emergency_exists = any(f.exists() for f in emergency_files)
        results["checks"].append(
            {
                "name": "Emergency Stop Mechanism",
                "status": "PASS" if emergency_exists else "WARNING",
                "message": (
                    "Emergency stop files found"
                    if emergency_exists
                    else "Emergency stop files not found"
                ),
            }
        )
        # Check 2: Safety mechanisms configuration
        safety_config = self.repo_path / "config/safety-mechanisms.yaml"
        if safety_config.exists():
            results["checks"].append(
                {
                    "name": "Safety Mechanisms Config",
                    "status": "PASS",
                    "message": f"Safety config exists at {safety_config}",
                }
            )
        else:
            results["checks"].append(
                {
                    "name": "Safety Mechanisms Config",
                    "status": "WARNING",
                    "message": "Safety mechanisms config not found",
                }
            )
        # Check 3: CI/CD workflows
        workflows_dir = self.repo_path / ".github/workflows"
        if workflows_dir.exists():
            from itertools import chain
            workflow_count = sum(
                1 for _ in chain(
                    workflows_dir.glob("*.yml"),
                    workflows_dir.glob("*.yaml")
                )
            )
            results["checks"].append(
                {
                    "name": "CI/CD Workflows",
                    "status": "PASS" if workflow_count > 0 else "WARNING",
                    "message": f"Found {workflow_count} workflow files",
                }
            )
        else:
            results["checks"].append(
                {
                    "name": "CI/CD Workflows",
                    "status": "FAIL",
                    "message": "No .github/workflows directory found",
                }
            )
        # Calculate summary
        for check in results["checks"]:
            if check["status"] == "PASS":
                results["passed"] += 1
            elif check["status"] == "FAIL":
                results["failed"] += 1
            else:
                results["warnings"] += 1
        self.logger.info(
            f"‚úÖ P0 Safety: {results['passed']} passed, "
            f"{results['failed']} failed, {results['warnings']} warnings"
        )
        return results
    # =========================================================================
    # Cleanup Execution
    # =========================================================================
    def execute_cleanup(
        self, phase: str = "all", dry_run: bool = True
    ) -> Dict[str, Any]:
        """
        Execute cleanup based on specified phase.
        Args:
            phase: One of 'duplicates', 'todos', 'stubs', or 'all'
            dry_run: If True, only show what would be done without making changes
        Returns:
            Dictionary with cleanup results
        """
        self.logger.info(f"üßπ Executing cleanup phase: {phase} (dry_run={dry_run})")
        results = {
            "phase": phase,
            "dry_run": dry_run,
            "timestamp": datetime.now().isoformat(),
            "actions": [],
            "files_modified": 0,
            "items_processed": 0,
        }
        if phase in ["duplicates", "all"]:
            dup_results = self._cleanup_duplicates(dry_run)
            results["actions"].extend(dup_results)
            results["items_processed"] += len(dup_results)
        if phase in ["todos", "all"]:
            todo_results = self._cleanup_todos(dry_run)
            results["actions"].extend(todo_results)
            results["items_processed"] += len(todo_results)
        if phase in ["stubs", "all"]:
            stub_results = self._cleanup_stubs(dry_run)
            results["actions"].extend(stub_results)
            results["items_processed"] += len(stub_results)
        if not dry_run:
            results["files_modified"] = len(
                set(a.get("file") for a in results["actions"] if a.get("executed"))
            )
        self.logger.info(
            f"‚úÖ Cleanup complete: {results['items_processed']} items processed"
        )
        return results
    def _cleanup_duplicates(self, dry_run: bool) -> List[Dict[str, Any]]:
        """Clean up duplicate files"""
        actions = []
        duplicates = self.find_duplicates()
        for group in duplicates:
            for removable_file in group.removable:
                action = {
                    "type": "remove_duplicate",
                    "file": removable_file,
                    "reason": f"Duplicate of {group.files[0]}",
                    "executed": False,
                }
                if not dry_run:
                    try:
                        file_path = self.repo_path / removable_file
                        if file_path.exists():
                            file_path.unlink()
                            action["executed"] = True
                            self.logger.info(f"Removed duplicate: {removable_file}")
                    except Exception as e:
                        action["error"] = str(e)
                        self.logger.warning(f"Failed to remove {removable_file}: {e}")
                actions.append(action)
        return actions
    def _cleanup_todos(self, dry_run: bool) -> List[Dict[str, Any]]:
        """Generate cleanup suggestions for TODOs (non-destructive)"""
        actions = []
        todos = self.find_todos()
        for todo in todos:
            action = {
                "type": "todo_suggestion",
                "file": todo.file_path,
                "line": todo.line_number,
                "todo_type": todo.todo_type,
                "severity": todo.severity,
                "message": todo.message,
                "suggestion": self._generate_todo_suggestion(todo),
                "executed": False,
            }
            actions.append(action)
        return actions
    def _generate_todo_suggestion(self, todo: TodoItem) -> str:
        """Generate a suggestion for addressing a TODO"""
        if todo.severity == "HIGH":
            return f"Priority fix needed: {todo.message[:50]}..."
        elif todo.todo_type == "DEPRECATED":
            return "Consider removing deprecated code or updating references"
        elif todo.todo_type == "HACK":
            return "Refactor this hack into a proper implementation"
        else:
            return f"Address TODO: {todo.message[:50]}..."
    def _cleanup_stubs(self, dry_run: bool) -> List[Dict[str, Any]]:
        """Generate cleanup suggestions for NotImplementedError stubs"""
        actions = []
        stubs = self.find_not_implemented_stubs()
        for stub in stubs:
            action = {
                "type": "stub_suggestion",
                "file": stub.file_path,
                "line": stub.line_number,
                "function": stub.function_name,
                "class": stub.class_name,
                "suggestion": f"Implement {stub.class_name + '.' if stub.class_name else ''}{stub.function_name}",
                "executed": False,
            }
            actions.append(action)
        return actions
    # =========================================================================
    # Git Workflow Automation (Phase 8)
    # =========================================================================
    def run_git_workflow(
        self, action: str = "status", message: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Run Git workflow automation.
        Args:
            action: One of 'status', 'stage', 'commit', 'branch'
            message: Commit message (required for commit action)
        Returns:
            Dictionary with workflow results
        """
        self.logger.info(f"üîÑ Running Git workflow: {action}")
        results = {
            "action": action,
            "timestamp": datetime.now().isoformat(),
            "success": False,
            "output": "",
        }
        try:
            if action == "status":
                result = subprocess.run(
                    ["git", "status", "--porcelain"],
                    capture_output=True,
                    text=True,
                    cwd=self.repo_path,
                    check=False,
                )
                results["output"] = result.stdout
                results["success"] = result.returncode == 0
                results["changed_files"] = len(
                    [line for line in result.stdout.split("\n") if line.strip()]
                )
            elif action == "stage":
                result = subprocess.run(
                    ["git", "add", "-A"],
                    capture_output=True,
                    text=True,
                    cwd=self.repo_path,
                    check=False,
                )
                results["success"] = result.returncode == 0
                results["output"] = result.stdout or "All changes staged"
            elif action == "branch":
                result = subprocess.run(
                    ["git", "branch", "--show-current"],
                    capture_output=True,
                    text=True,
                    cwd=self.repo_path,
                    check=False,
                )
                results["success"] = result.returncode == 0
                results["current_branch"] = result.stdout.strip()
                results["output"] = f"Current branch: {results['current_branch']}"
            else:
                results["output"] = f"Unknown action: {action}"
        except Exception as e:
            results["error"] = str(e)
            self.logger.warning(f"Git workflow error: {e}")
        return results
    # =========================================================================
    # Report Generation
    # =========================================================================
    def generate_report(self, output_path: Optional[Path] = None) -> CleanupReport:
        """Generate comprehensive cleanup report"""
        self.logger.info("üìä Generating cleanup report...")
        # Scan all categories
        duplicates = self.find_duplicates()
        todos = self.find_todos()
        stubs = self.find_not_implemented_stubs()
        # Create report
        report = CleanupReport(
            timestamp=datetime.now().isoformat(),
            phase="Analysis",
            items_found=len(duplicates) + len(todos) + len(stubs),
            items_fixed=0,
            items_remaining=len(duplicates) + len(todos) + len(stubs),
            files_modified=0,
            lines_added=0,
            lines_removed=0,
            details={
                "duplicates": {
                    "groups": len(duplicates),
                    "total_files": sum(len(g.files) for g in duplicates),
                    "removable": sum(len(g.removable) for g in duplicates),
                    "potential_savings_kb": sum(g.size_bytes for g in duplicates)
                    / 1024,
                },
                "todos": {
                    "total": len(todos),
                    "by_severity": {
                        "HIGH": len([t for t in todos if t.severity == "HIGH"]),
                        "MEDIUM": len([t for t in todos if t.severity == "MEDIUM"]),
                        "LOW": len([t for t in todos if t.severity == "LOW"]),
                    },
                    "by_type": {
                        "TODO": len([t for t in todos if t.todo_type == "TODO"]),
                        "FIXME": len([t for t in todos if t.todo_type == "FIXME"]),
                        "HACK": len([t for t in todos if t.todo_type == "HACK"]),
                        "DEPRECATED": len(
                            [t for t in todos if t.todo_type == "DEPRECATED"]
                        ),
                    },
                },
                "not_implemented": {
                    "total": len(stubs),
                    "files": len(set(s.file_path for s in stubs)),
                },
            },
        )
        # Save to file if requested
        if output_path:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, "w", encoding='utf-8') as f:
                json.dump(asdict(report), f, indent=2, ensure_ascii=False)
            self.logger.info(f"üìÑ Report saved to {output_path}")
        # Print summary
        self._print_report_summary(report)
        return report
    def _print_report_summary(self, report: CleanupReport):
        """Print colorful report summary to console"""
        print("\n" + "=" * 70)
        print(f"{Colors.BOLD}{Colors.CYAN}üìä Autonomous Cleanup Report{Colors.END}")
        print("=" * 70)
        print(f"Timestamp: {report.timestamp}")
        print(f"Phase: {report.phase}")
        print(f"\n{Colors.BOLD}Summary:{Colors.END}")
        print(f"  Items Found: {Colors.YELLOW}{report.items_found}{Colors.END}")
        print(f"  Items Fixed: {Colors.GREEN}{report.items_fixed}{Colors.END}")
        print(f"  Items Remaining: {Colors.RED}{report.items_remaining}{Colors.END}")
        # Duplicates
        print(f"\n{Colors.BOLD}üìÇ Duplicates:{Colors.END}")
        dup_details = report.details.get("duplicates", {})
        print(f"  Groups: {dup_details.get('groups', 0)}")
        print(f"  Total Files: {dup_details.get('total_files', 0)}")
        print(
            f"  Removable: {Colors.GREEN}{dup_details.get('removable', 0)}{Colors.END}"
        )
        print(
            f"  Potential Savings: {dup_details.get('potential_savings_kb', 0):.2f} KB"
        )
        # TODOs
        print(f"\n{Colors.BOLD}üìù TODOs:{Colors.END}")
        todo_details = report.details.get("todos", {})
        print(f"  Total: {todo_details.get('total', 0)}")
        print("  By Severity:")
        severity = todo_details.get("by_severity", {})
        print(f"    HIGH: {Colors.RED}{severity.get('HIGH', 0)}{Colors.END}")
        print(f"    MEDIUM: {Colors.YELLOW}{severity.get('MEDIUM', 0)}{Colors.END}")
        print(f"    LOW: {Colors.GREEN}{severity.get('LOW', 0)}{Colors.END}")
        # NotImplemented
        print(f"\n{Colors.BOLD}üöß NotImplementedError:{Colors.END}")
        ni_details = report.details.get("not_implemented", {})
        print(f"  Total Stubs: {ni_details.get('total', 0)}")
        print(f"  Files Affected: {ni_details.get('files', 0)}")
        print("=" * 70 + "\n")
# =============================================================================
# CLI Interface
# =============================================================================
def main():
    parser = argparse.ArgumentParser(
        description="Autonomous Cleanup Toolkit - Claude Code Capabilities"
    )
    subparsers = parser.add_subparsers(dest="command", help="Commands")
    # Analyze command
    analyze_parser = subparsers.add_parser("analyze", help="Run full analysis")
    analyze_parser.add_argument(
        "--output",
        type=Path,
        default=Path("CLEANUP_ANALYSIS_REPORT.json"),
        help="Output file for report",
    )
    # Cleanup command
    cleanup_parser = subparsers.add_parser("cleanup", help="Execute cleanup")
    cleanup_parser.add_argument(
        "--phase",
        choices=["duplicates", "todos", "stubs", "all"],
        default="all",
        help="Cleanup phase to execute",
    )
    cleanup_parser.add_argument(
        "--execute",
        action="store_true",
        help="Actually execute cleanup (default is dry-run mode)",
    )
    # Report command
    report_parser = subparsers.add_parser("report", help="Generate report only")
    report_parser.add_argument(
        "--output",
        type=Path,
        default=Path("CLEANUP_REPORT.json"),
        help="Output file for report",
    )
    # P0 Safety verification command
    safety_parser = subparsers.add_parser("safety", help="Run P0 safety verification")
    safety_parser.add_argument(
        "--output",
        type=Path,
        help="Output file for safety report",
    )
    # Git workflow command
    git_parser = subparsers.add_parser("git", help="Git workflow automation")
    git_parser.add_argument(
        "--action",
        choices=["status", "stage", "branch"],
        default="status",
        help="Git action to perform",
    )
    args = parser.parse_args()
    # Initialize engine
    repo_path = Path.cwd()
    engine = AutonomousCleanupEngine(repo_path)
    # Execute command
    if args.command == "analyze":
        engine.generate_report(output_path=args.output)
        print(f"\n‚úÖ Analysis complete. Report saved to {args.output}")
    elif args.command == "report":
        engine.generate_report(output_path=args.output)
        print(f"\n‚úÖ Report generated: {args.output}")
    elif args.command == "cleanup":
        dry_run = not args.execute
        print(f"üßπ Cleanup phase: {args.phase}")
        if dry_run:
            print("üîç DRY RUN MODE - No changes will be made")
        else:
            print("‚ö†Ô∏è  EXECUTE MODE - Changes will be applied!")
        results = engine.execute_cleanup(phase=args.phase, dry_run=dry_run)
        print("\nüìä Cleanup Results:")
        print(f"  Items Processed: {results['items_processed']}")
        print(f"  Files Modified: {results['files_modified']}")
        # Show sample actions
        if results["actions"]:
            print("\nüìù Sample Actions (showing first 5):")
            for action in results["actions"][:5]:
                status = "‚úÖ" if action.get("executed") else "üìã"
                print(f"  {status} [{action['type']}] {action.get('file', 'N/A')}")
    elif args.command == "safety":
        results = engine.verify_p0_safety()
        print("\nüîí P0 Safety Verification Results:")
        print(f"  Passed: {results['passed']}")
        print(f"  Failed: {results['failed']}")
        print(f"  Warnings: {results['warnings']}")
        status_emojis = {"PASS": "‚úÖ", "FAIL": "‚ùå", "WARNING": "‚ö†Ô∏è"}
        for check in results["checks"]:
            status_emoji = status_emojis.get(check["status"], "‚ùì")
            print(f"  {status_emoji} {check['name']}: {check['message']}")
        if args.output:
            with open(args.output, "w", encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"\nüìÑ Safety report saved to {args.output}")
    elif args.command == "git":
        results = engine.run_git_workflow(action=args.action)
        print(f"\nüîÑ Git Workflow: {args.action}")
        print(f"  Success: {'‚úÖ' if results['success'] else '‚ùå'}")
        print(f"  Output: {results['output']}")
    else:
        parser.print_help()
if __name__ == "__main__":
    main()
