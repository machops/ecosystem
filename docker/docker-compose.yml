# IndestructibleEco v1.0 — Full Production Stack
# URI: indestructibleeco://docker/compose/production
# Usage: cd docker && docker compose up
# Context: repository root (..)

version: "3.9"

x-common: &common
  restart: unless-stopped
  logging:
    driver: json-file
    options:
      max-size: "50m"
      max-file: "5"

services:
  # ── API Gateway (Python FastAPI) ────────────────────────────────────
  api-gateway:
    <<: *common
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: eco-api-gateway
    ports:
      - "8000:8000"
    environment:
      - ECO_ENVIRONMENT=production
      - ECO_LOG_LEVEL=INFO
      - ECO_REDIS_HOST=redis
      - ECO_POSTGRES_HOST=postgres
      - ECO_VLLM_HOST=vllm
      - ECO_VLLM_PORT=8001
      - ECO_TGI_HOST=tgi
      - ECO_TGI_PORT=8002
      - ECO_SGLANG_HOST=sglang
      - ECO_SGLANG_PORT=8003
      - ECO_OLLAMA_HOST=ollama
      - ECO_OLLAMA_PORT=11434
    env_file:
      - ../.env
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    networks:
      - eco-net
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8000/health').raise_for_status()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ── vLLM Engine ─────────────────────────────────────────────────────
  vllm:
    <<: *common
    build:
      context: ..
      dockerfile: docker/Dockerfile.gpu
      target: vllm
    container_name: eco-vllm
    ports:
      - "8001:8001"
    volumes:
      - model-cache:/models
    environment:
      - HF_HOME=/models
      - VLLM_ATTENTION_BACKEND=FLASH_ATTN
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - eco-net

  # ── TGI Engine ──────────────────────────────────────────────────────
  tgi:
    <<: *common
    image: ghcr.io/huggingface/text-generation-inference:2.4.1
    container_name: eco-tgi
    ports:
      - "8002:8002"
    volumes:
      - model-cache:/models
    environment:
      - MODEL_ID=meta-llama/Llama-3.1-8B-Instruct
      - PORT=8002
      - MAX_INPUT_LENGTH=4096
      - MAX_TOTAL_TOKENS=8192
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
    command: ["--model-id", "meta-llama/Llama-3.1-8B-Instruct", "--port", "8002"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - eco-net

  # ── SGLang Engine ───────────────────────────────────────────────────
  sglang:
    <<: *common
    build:
      context: ..
      dockerfile: docker/Dockerfile.gpu
      target: sglang
    container_name: eco-sglang
    ports:
      - "8003:8003"
    volumes:
      - model-cache:/models
    environment:
      - HF_HOME=/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - eco-net

  # ── Ollama Engine ───────────────────────────────────────────────────
  ollama:
    <<: *common
    image: ollama/ollama:latest
    container_name: eco-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - eco-net

  # ── Redis ───────────────────────────────────────────────────────────
  redis:
    <<: *common
    image: redis:7-alpine
    container_name: eco-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - eco-net

  # ── PostgreSQL ──────────────────────────────────────────────────────
  postgres:
    <<: *common
    image: postgres:16-alpine
    container_name: eco-postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=indestructibleeco
      - POSTGRES_PASSWORD=${ECO_POSTGRES_PASSWORD:-eco_secret}
      - POSTGRES_DB=indestructibleeco
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U indestructibleeco"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - eco-net

  # ── Prometheus ──────────────────────────────────────────────────────
  prometheus:
    <<: *common
    image: prom/prometheus:v2.54.0
    container_name: eco-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.retention.time=30d"
      - "--web.enable-lifecycle"
    networks:
      - eco-net

  # ── Grafana ─────────────────────────────────────────────────────────
  grafana:
    <<: *common
    image: grafana/grafana:11.2.0
    container_name: eco-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${ECO_SECRET_KEY:-eco_admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - eco-net

volumes:
  model-cache:
    driver: local
  ollama-data:
    driver: local
  redis-data:
    driver: local
  postgres-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

networks:
  eco-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16