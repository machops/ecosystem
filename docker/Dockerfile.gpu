# IndestructibleEco v1.0 — GPU Inference Engine Images (Multi-Stage)
# URI: indestructibleeco://docker/gpu-engines
# Targets: vllm, sglang, tgi
# Build: docker build -t ghcr.io/indestructibleorg/vllm:v1.0.0 -f docker/Dockerfile.gpu --target vllm .

# ── Stage: Base (CUDA 12.4 + Python 3.11) ────────────────────────────
FROM nvidia/cuda:12.4.1-devel-ubuntu22.04 AS base

LABEL org.opencontainers.image.title="IndestructibleEco GPU Engine Base" \
      org.opencontainers.image.version="1.0.0" \
      org.opencontainers.image.vendor="IndestructibleEco" \
      org.opencontainers.image.source="https://github.com/indestructibleorg/indestructibleeco"

RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.11 python3-pip python3.11-venv curl && \
    rm -rf /var/lib/apt/lists/*

RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 && \
    update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1

WORKDIR /engine

# ── Target: vLLM ──────────────────────────────────────────────────────
FROM base AS vllm

RUN pip install --no-cache-dir \
    vllm>=0.6.6 \
    ray>=2.38.0 \
    flash-attn>=2.6.0

EXPOSE 8001

ENTRYPOINT ["python", "-m", "vllm.entrypoints.openai.api_server"]
CMD ["--host", "0.0.0.0", \
     "--port", "8001", \
     "--model", "meta-llama/Llama-3.1-8B-Instruct", \
     "--gpu-memory-utilization", "0.90", \
     "--max-model-len", "32768", \
     "--enable-prefix-caching", \
     "--enable-chunked-prefill", \
     "--disable-log-requests"]

# ── Target: SGLang ────────────────────────────────────────────────────
FROM base AS sglang

RUN pip install --no-cache-dir \
    "sglang[all]>=0.3.6"

EXPOSE 8003

ENTRYPOINT ["python", "-m", "sglang.launch_server"]
CMD ["--host", "0.0.0.0", \
     "--port", "8003", \
     "--model-path", "meta-llama/Llama-3.1-8B-Instruct", \
     "--mem-fraction-static", "0.88"]

# ── Target: TGI ───────────────────────────────────────────────────────
FROM ghcr.io/huggingface/text-generation-inference:2.4.1 AS tgi

EXPOSE 8002

ENTRYPOINT ["text-generation-launcher"]
CMD ["--model-id", "meta-llama/Llama-3.1-8B-Instruct", \
     "--port", "8002", \
     "--max-input-length", "4096", \
     "--max-total-tokens", "8192"]