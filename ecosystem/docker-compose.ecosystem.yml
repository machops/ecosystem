# ╔══════════════════════════════════════════════════════════════════════════════╗
# ║  indestructibleeco v1.0 — docker-compose.ecosystem.yml                     ║
# ║  Observability & Service Discovery Stack                                    ║
# ║                                                                              ║
# ║  Usage:                                                                      ║
# ║    docker compose -f docker-compose.ecosystem.yml up -d                     ║
# ║    docker compose -f docker-compose.ecosystem.yml logs -f grafana           ║
# ║    docker compose -f docker-compose.ecosystem.yml down                      ║
# ║                                                                              ║
# ║  Or via root alias (package.json):                                           ║
# ║    pnpm ecosystem:up                                                         ║
# ║    pnpm ecosystem:down                                                       ║
# ║                                                                              ║
# ║  Access points after startup:                                                ║
# ║    Grafana          http://localhost:3030   admin / admin                    ║
# ║    Prometheus       http://localhost:9090                                    ║
# ║    Alertmanager     http://localhost:9093                                    ║
# ║    Jaeger UI        http://localhost:16686                                   ║
# ║    Consul UI        http://localhost:8500                                    ║
# ║    Loki (via Grafana datasource)                                             ║
# ║    cAdvisor         http://localhost:8888                                    ║
# ╚══════════════════════════════════════════════════════════════════════════════╝

version: "3.9"

# ── Shared extension fields ──────────────────────────────────────────────────
x-logging: &default-logging
  driver: json-file
  options:
    max-size: "10m"
    max-file: "3"

x-restart: &always-restart
  restart: unless-stopped

x-healthcheck-defaults: &hc-defaults
  interval: 20s
  timeout: 5s
  retries: 5
  start_period: 15s

# ── Networks ──────────────────────────────────────────────────────────────────
networks:
  observability:
    driver: bridge
    name: eco_observability
  # Attach to the main backend network to scrape service metrics
  backend:
    external: true
    name: eco_backend

# ── Volumes ───────────────────────────────────────────────────────────────────
volumes:
  prometheus-data:
    name: eco_prometheus_data
  alertmanager-data:
    name: eco_alertmanager_data
  grafana-data:
    name: eco_grafana_data
  loki-data:
    name: eco_loki_data
  tempo-data:
    name: eco_tempo_data
  consul-data:
    name: eco_consul_data

# ══════════════════════════════════════════════════════════════════════════════
# SERVICES
# ══════════════════════════════════════════════════════════════════════════════
services:

  # ── METRICS ─────────────────────────────────────────────────────────────────

  # Prometheus — Metrics scraper & TSDB
  prometheus:
    image: prom/prometheus:v2.53.0
    container_name: eco_prometheus
    <<: *always-restart
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.retention.time=7d
      - --storage.tsdb.retention.size=2GB
      - --web.enable-lifecycle
      - --web.enable-admin-api
      - --web.console.libraries=/usr/share/prometheus/console_libraries
      - --web.console.templates=/usr/share/prometheus/consoles
    volumes:
      - ./ecosystem/monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./ecosystem/monitoring/prometheus/alerts:/etc/prometheus/alerts:ro
      - prometheus-data:/prometheus
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    networks:
      - observability
      - backend
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:9090/-/healthy || exit 1"]
      <<: *hc-defaults
    logging: *default-logging
    labels:
      generated-by: yaml-toolkit-v8
      service: prometheus
      env: development
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1.0"

  # Alertmanager — Alert routing & deduplication
  alertmanager:
    image: prom/alertmanager:v0.27.0
    container_name: eco_alertmanager
    <<: *always-restart
    command:
      - --config.file=/etc/alertmanager/alertmanager.yml
      - --storage.path=/alertmanager
      - --web.external-url=http://localhost:9093
    volumes:
      - ./ecosystem/monitoring/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager-data:/alertmanager
    ports:
      - "${ALERTMANAGER_PORT:-9093}:9093"
    networks:
      - observability
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:9093/-/healthy || exit 1"]
      <<: *hc-defaults
    logging: *default-logging
    labels:
      generated-by: yaml-toolkit-v8
      service: alertmanager
      env: development

  # Node Exporter — Host metrics (CPU, RAM, disk, network)
  node-exporter:
    image: prom/node-exporter:v1.8.1
    container_name: eco_node_exporter
    <<: *always-restart
    command:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/rootfs
      - --collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    network_mode: host
    pid: host
    logging: *default-logging
    labels:
      generated-by: yaml-toolkit-v8
      service: node-exporter
      env: development

  # cAdvisor — Container metrics
  # SECURITY NOTE: cAdvisor requires privileged mode to access host system metrics.
  # This is a documented exception for observability infrastructure.
  # In production, consider:
  # 1. Running cAdvisor on a dedicated monitoring node
  # 2. Using alternative metrics collection (e.g., kubelet metrics in GKE)
  # 3. Restricting network access to cAdvisor endpoint
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.49.1
    container_name: eco_cadvisor
    <<: *always-restart
    privileged: true
    command:
      - -housekeeping_interval=15s
      - -docker_only=true
      - -disable_metrics=percpu,sched,tcp,udp,disk,diskIO,network
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /dev/disk:/dev/disk:ro
    ports:
      - "${CADVISOR_PORT:-8888}:8080"
    networks:
      - observability
    logging: *default-logging
    labels:
      generated-by: yaml-toolkit-v8
      service: cadvisor
      env: development

  # Redis Exporter — Redis metrics for Prometheus
  redis-exporter:
    image: oliver006/redis_exporter:v1.62.0
    container_name: eco_redis_exporter
    <<: *always-restart
    environment:
      REDIS_ADDR:     redis://redis:6379
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
    networks:
      - observability
      - backend
    logging: *default-logging
    labels:
      generated-by: yaml-toolkit-v8
      service: redis-exporter
      env: development

  # Postgres Exporter — PostgreSQL metrics
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.15.0
    container_name: eco_postgres_exporter
    <<: *always-restart
    environment:
      DATA_SOURCE_NAME: >-
        postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}
        @postgres:5432/${POSTGRES_DB:-indestructibleeco}?sslmode=disable
    networks:
      - observability
      - backend
    logging: *default-logging
    labels:
      generated-by: yaml-toolkit-v8
      service: postgres-exporter
      env: development

  # ── DASHBOARDS ───────────────────────────────────────────────────────────────

  # Grafana — Metrics + Logs + Traces dashboards
  grafana:
    image: grafana/grafana:11.1.3
    container_name: eco_grafana
    <<: *always-restart
    environment:
      # Admin
      GF_SECURITY_ADMIN_USER:       ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD:   ${GRAFANA_ADMIN_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP:       "false"
      GF_USERS_DEFAULT_THEME:       dark

      # Server
      GF_SERVER_ROOT_URL:           http://localhost:${GRAFANA_PORT:-3030}
      GF_SERVER_SERVE_FROM_SUB_PATH: "false"

      # Datasource provisioning
      GF_PATHS_PROVISIONING:        /etc/grafana/provisioning

      # Plugins
      GF_INSTALL_PLUGINS: >-
        grafana-clock-panel,
        grafana-simple-json-datasource,
        grafana-piechart-panel,
        grafana-worldmap-panel

      # Feature flags
      GF_FEATURE_TOGGLES_ENABLE: tempoSearch,tempoBackendSearch

      # Anonymous access (dev convenience)
      GF_AUTH_ANONYMOUS_ENABLED:     "true"
      GF_AUTH_ANONYMOUS_ORG_ROLE:    Viewer
    volumes:
      - grafana-data:/var/lib/grafana
      - ./ecosystem/monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./ecosystem/monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "${GRAFANA_PORT:-3030}:3000"
    depends_on:
      prometheus:
        condition: service_healthy
    networks:
      - observability
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3000/api/health || exit 1"]
      <<: *hc-defaults
    logging: *default-logging
    labels:
      generated-by: yaml-toolkit-v8
      service: grafana
      env: development
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"

  # ── LOGGING ──────────────────────────────────────────────────────────────────

  # Loki — Log aggregation backend
  loki:
    image: grafana/loki:3.1.0
    container_name: eco_loki
    <<: *always-restart
    command: -config.file=/etc/loki/loki.yml
    volumes:
      - ./ecosystem/logging/loki.yml:/etc/loki/loki.yml:ro
      - loki-data:/loki
    ports:
      - "${LOKI_PORT:-3100}:3100"
    networks:
      - observability
      - backend
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3100/ready || exit 1"]
      <<: *hc-defaults
    logging: *default-logging
    labels:
      generated-by: yaml-toolkit-v8
      service: loki
      env: development

  # Promtail — Log collector (ship Docker logs → Loki)
  # SECURITY NOTE: Promtail requires Docker socket access to collect container logs.
  # This is a documented exception for observability infrastructure.
  # In production, consider:
  # 1. Using read-only Docker socket mount
  # 2. Restricting Promtail's access to specific log files
  # 3. Using alternative log collection (e.g., Fluent Bit with limited permissions)
  promtail:
    image: grafana/promtail:3.1.0
    container_name: eco_promtail
    <<: *always-restart
    command: -config.file=/etc/promtail/promtail.yml
    volumes:
      - ./ecosystem/logging/promtail.yml:/etc/promtail/promtail.yml:ro
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      loki:
        condition: service_healthy
    networks:
      - observability
    logging: *default-logging
    labels:
      generated-by: yaml-toolkit-v8
      service: promtail
      env: development

  # ── TRACING ──────────────────────────────────────────────────────────────────

  # Tempo — Distributed trace backend
  tempo:
    image: grafana/tempo:2.5.0
    container_name: eco_tempo
    <<: *always-restart
    command:
      - -config.file=/etc/tempo/tempo.yml
    volumes:
      - ./ecosystem/tracing/tempo.yml:/etc/tempo/tempo.yml:ro
      - tempo-data:/tmp/tempo
    ports:
      - "${TEMPO_PORT:-3200}:3200"     # Tempo HTTP API
      - "9411:9411"                    # Zipkin receiver
    networks:
      - observability
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3200/ready || exit 1"]
      <<: *hc-defaults
    logging: *default-logging
    labels:
      generated-by: yaml-toolkit-v8
      service: tempo
      env: development

  # Jaeger — Trace UI (connects to Tempo or standalone)
  jaeger:
    image: jaegertracing/all-in-one:1.60
    container_name: eco_jaeger
    <<: *always-restart
    environment:
      COLLECTOR_OTLP_ENABLED:     "true"
      COLLECTOR_ZIPKIN_HOST_PORT: ":9411"
      SPAN_STORAGE_TYPE:          memory
      MEMORY_MAX_TRACES:          "10000"
      LOG_LEVEL:                  info
    ports:
      - "${JAEGER_UI_PORT:-16686}:16686"    # Jaeger UI
      - "14250:14250"                        # gRPC model.proto
      - "14268:14268"                        # HTTP thrift
      - "${JAEGER_OTLP_GRPC_PORT:-4317}:4317"
      - "${JAEGER_OTLP_HTTP_PORT:-4318}:4318"
      - "9411:9411"                          # Zipkin
    networks:
      - observability
      - backend
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:16686/ || exit 1"]
      <<: *hc-defaults
    logging: *default-logging
    labels:
      generated-by: yaml-toolkit-v8
      service: jaeger
      env: development
    deploy:
      resources:
        limits:
          memory: 512M

  # OpenTelemetry Collector — Unified telemetry pipeline
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.104.0
    container_name: eco_otel_collector
    <<: *always-restart
    command:
      - --config=/etc/otelcol/config.yml
    volumes:
      - ./ecosystem/tracing/otel-collector.yml:/etc/otelcol/config.yml:ro
    ports:
      - "4317:4317"     # OTLP gRPC receiver
      - "4318:4318"     # OTLP HTTP receiver
      - "8889:8889"     # Prometheus metrics exporter (self-telemetry)
      - "55679:55679"   # zpages extension
    depends_on:
      jaeger:
        condition: service_healthy
      loki:
        condition: service_healthy
    networks:
      - observability
      - backend
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:13133/"]
      <<: *hc-defaults
    logging: *default-logging
    labels:
      generated-by: yaml-toolkit-v8
      service: otel-collector
      env: development

  # ── SERVICE DISCOVERY ────────────────────────────────────────────────────────

  # Consul — Service registry + health checks + KV store
  consul:
    image: hashicorp/consul:1.19
    container_name: eco_consul
    <<: *always-restart
    command:
      - agent
      - -server
      - -bootstrap
      - -ui
      - -client=0.0.0.0
      - -bind=0.0.0.0
      - -advertise=127.0.0.1
      - -data-dir=/consul/data
      - -config-dir=/consul/config
      - -log-level=info
      - -enable-script-checks=false
    environment:
      CONSUL_BIND_INTERFACE: eth0
    volumes:
      - consul-data:/consul/data
      - ./ecosystem/service-discovery/consul/config:/consul/config:ro
    ports:
      - "${CONSUL_UI_PORT:-8500}:8500"    # HTTP + UI
      - "8501:8501"                        # HTTPS
      - "8600:8600/udp"                    # DNS
      - "8300:8300"                        # Server RPC
    networks:
      - observability
      - backend
    healthcheck:
      test: ["CMD", "consul", "members"]
      <<: *hc-defaults
    logging: *default-logging
    labels:
      generated-by: yaml-toolkit-v8
      service: consul
      env: development
