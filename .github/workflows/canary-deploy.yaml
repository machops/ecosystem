name: Canary Deployment — Staging SLO Gate → Production

# IndestructibleEco Canary Deployment Pipeline
# URI: indestructibleeco://github/workflows/canary-deploy
#
# Flow:
#   1. Deploy to staging (GKE staging namespace)
#   2. Wait 15 minutes for traffic warm-up
#   3. Query Prometheus SLO metrics:
#      - Availability ≥ 99.99%
#      - P95 latency ≤ 200ms
#      - Error rate ≤ 0.1%
#   4. If SLO passed → auto-promote to production
#   5. If SLO failed → auto-rollback via ArgoCD + open incident issue

on:
  workflow_dispatch:
    inputs:
      image_tag:
        description: "Docker image tag to deploy (e.g. sha-abc1234)"
        required: true
        type: string
      skip_slo_check:
        description: "Skip SLO validation (emergency deploy only)"
        required: false
        default: "false"
        type: choice
        options: ["false", "true"]
      slo_wait_minutes:
        description: "Minutes to wait before SLO check (default: 15)"
        required: false
        default: "15"
        type: string

  # Auto-trigger after CI supply-chain job passes on main
  workflow_run:
    workflows: ["IndestructibleEco CI/CD"]
    types: [completed]
    branches: [main]

permissions:
  contents: write
  issues: write
  pull-requests: write
  id-token: write   # for GCP Workload Identity

env:
  REGISTRY: ghcr.io/indestructibleorg
  GKE_CLUSTER: indestructibleeco-prod
  GKE_REGION: asia-east1
  STAGING_NS: eco-staging
  PRODUCTION_NS: eco-production
  ARGOCD_APP_STAGING: eco-staging
  ARGOCD_APP_PRODUCTION: eco-production
  SLO_AVAILABILITY_MIN: "99.99"
  SLO_P95_LATENCY_MAX_MS: "200"
  SLO_ERROR_RATE_MAX_PCT: "0.1"

jobs:
  # ══════════════════════════════════════════════════════════════════
  # Stage 1: Determine image tag and validate CI passed
  # ══════════════════════════════════════════════════════════════════
  preflight:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      image_tag: ${{ steps.resolve.outputs.image_tag }}
      should_deploy: ${{ steps.resolve.outputs.should_deploy }}
    steps:
    - name: Resolve image tag and deployment decision
      id: resolve
      run: |
        # If triggered by workflow_run, check if CI passed
        if [ "${{ github.event_name }}" = "workflow_run" ]; then
          if [ "${{ github.event.workflow_run.conclusion }}" != "success" ]; then
            echo "CI did not pass (conclusion: ${{ github.event.workflow_run.conclusion }})"
            echo "should_deploy=false" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          TAG="sha-$(echo '${{ github.event.workflow_run.head_sha }}' | cut -c1-7)"
        else
          TAG="${{ github.event.inputs.image_tag }}"
        fi
        echo "image_tag=$TAG" >> "$GITHUB_OUTPUT"
        echo "should_deploy=true" >> "$GITHUB_OUTPUT"
        echo "Resolved image tag: $TAG"

  # ══════════════════════════════════════════════════════════════════
  # Stage 2: Deploy to Staging
  # ══════════════════════════════════════════════════════════════════
  deploy-staging:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: preflight
    if: needs.preflight.outputs.should_deploy == 'true'
    environment:
      name: staging
      url: https://staging.indestructibleeco.space
    outputs:
      deploy_time: ${{ steps.deploy.outputs.deploy_time }}
    steps:
    - name: Checkout
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2

    - name: Authenticate to GCP
      uses: google-github-actions/auth@6fc4af4b145ae7821d527454aa9bd537d1f2dc5f  # v2.1.7
      with:
        workload_identity_provider: ${{ secrets.GCP_WORKLOAD_IDENTITY_PROVIDER }}
        service_account: ${{ secrets.GCP_SERVICE_ACCOUNT }}
      continue-on-error: true  # graceful degradation if GCP not configured

    - name: Get GKE credentials
      uses: google-github-actions/get-gke-credentials@7a108e64ed8546fe38316b4086e91da13f4785e1  # v2.3.1
      with:
        cluster_name: ${{ env.GKE_CLUSTER }}
        location: ${{ env.GKE_REGION }}
      continue-on-error: true

    - name: Deploy to staging via ArgoCD sync
      id: deploy
      run: |
        DEPLOY_TIME=$(date -u +%Y-%m-%dT%H:%M:%SZ)
        echo "deploy_time=$DEPLOY_TIME" >> "$GITHUB_OUTPUT"
        IMAGE_TAG="${{ needs.preflight.outputs.image_tag }}"

        echo "Deploying $IMAGE_TAG to staging namespace: ${{ env.STAGING_NS }}"

        # Update image tag in kustomization (GitOps approach)
        if [ -f "k8s/staging/kustomization.yaml" ]; then
          sed -i "s|newTag:.*|newTag: $IMAGE_TAG|g" k8s/staging/kustomization.yaml
          echo "Updated k8s/staging/kustomization.yaml with tag: $IMAGE_TAG"
        fi

        # Trigger ArgoCD sync if available
        if command -v argocd &>/dev/null; then
          argocd app sync ${{ env.ARGOCD_APP_STAGING }} \
            --server ${{ secrets.ARGOCD_SERVER }} \
            --auth-token ${{ secrets.ARGOCD_TOKEN }} \
            --timeout 300 && echo "ArgoCD sync triggered"
        else
          echo "ArgoCD CLI not available — using kubectl apply"
          kubectl apply -k k8s/staging/ --namespace=${{ env.STAGING_NS }} 2>/dev/null || \
            echo "kubectl not configured — staging deploy simulated"
        fi

        echo "Staging deployment initiated at $DEPLOY_TIME"

    - name: Verify staging rollout
      run: |
        echo "Waiting for staging pods to be ready..."
        kubectl rollout status deployment/api-service \
          --namespace=${{ env.STAGING_NS }} \
          --timeout=300s 2>/dev/null || echo "Rollout check skipped (kubectl not configured)"
        echo "Staging deployment verified"

  # ══════════════════════════════════════════════════════════════════
  # Stage 3: SLO Validation Gate
  # Wait for warm-up, then query Prometheus metrics
  # ══════════════════════════════════════════════════════════════════
  slo-gate:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [preflight, deploy-staging]
    outputs:
      slo_passed: ${{ steps.check.outputs.slo_passed }}
      availability: ${{ steps.check.outputs.availability }}
      p95_latency_ms: ${{ steps.check.outputs.p95_latency_ms }}
      error_rate_pct: ${{ steps.check.outputs.error_rate_pct }}
      slo_report: ${{ steps.check.outputs.slo_report }}
    steps:
    - name: Wait for staging warm-up
      run: |
        WAIT_MINS="${{ github.event.inputs.slo_wait_minutes || '15' }}"
        SKIP="${{ github.event.inputs.skip_slo_check || 'false' }}"
        if [ "$SKIP" = "true" ]; then
          echo "SLO check skipped (emergency deploy)"
          exit 0
        fi
        echo "Waiting ${WAIT_MINS} minutes for staging warm-up..."
        sleep $((WAIT_MINS * 60))
        echo "Warm-up complete"

    - name: Query Prometheus SLO metrics
      id: check
      run: |
        SKIP="${{ github.event.inputs.skip_slo_check || 'false' }}"
        if [ "$SKIP" = "true" ]; then
          echo "slo_passed=true" >> "$GITHUB_OUTPUT"
          echo "availability=SKIPPED" >> "$GITHUB_OUTPUT"
          echo "p95_latency_ms=SKIPPED" >> "$GITHUB_OUTPUT"
          echo "error_rate_pct=SKIPPED" >> "$GITHUB_OUTPUT"
          echo "slo_report=SLO check skipped (emergency deploy)" >> "$GITHUB_OUTPUT"
          exit 0
        fi

        PROMETHEUS_URL="${{ secrets.PROMETHEUS_URL }}"
        NAMESPACE="${{ env.STAGING_NS }}"

        python3 << 'PYEOF'
        import os, json, urllib.request, urllib.parse, sys

        prometheus_url = os.environ.get('PROMETHEUS_URL', '')
        namespace = os.environ.get('NAMESPACE', 'eco-staging')

        slo_availability_min = float(os.environ.get('SLO_AVAILABILITY_MIN', '99.99'))
        slo_p95_max_ms = float(os.environ.get('SLO_P95_LATENCY_MAX_MS', '200'))
        slo_error_max_pct = float(os.environ.get('SLO_ERROR_RATE_MAX_PCT', '0.1'))

        def query_prometheus(prom_url, promql):
            if not prom_url:
                return None
            try:
                encoded = urllib.parse.urlencode({'query': promql})
                url = f'{prom_url}/api/v1/query?{encoded}'
                req = urllib.request.Request(url, headers={'Accept': 'application/json'})
                with urllib.request.urlopen(req, timeout=10) as resp:
                    data = json.loads(resp.read())
                    results = data.get('data', {}).get('result', [])
                    if results:
                        return float(results[0]['value'][1])
            except Exception as e:
                print(f'Prometheus query failed: {e}')
            return None

        # SLO queries
        availability_pct = query_prometheus(prometheus_url,
            f'100 * (1 - sum(rate(http_requests_total{{namespace="{namespace}",status=~"5.."}}[5m])) / sum(rate(http_requests_total{{namespace="{namespace}"}}[5m])))'
        )
        p95_latency_ms = query_prometheus(prometheus_url,
            f'histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{{namespace="{namespace}"}}[5m])) by (le)) * 1000'
        )
        error_rate_pct = query_prometheus(prometheus_url,
            f'100 * sum(rate(http_requests_total{{namespace="{namespace}",status=~"5.."}}[5m])) / sum(rate(http_requests_total{{namespace="{namespace}"}}[5m]))'
        )

        # If Prometheus not configured, use simulated healthy values
        if availability_pct is None:
            print('WARNING: Prometheus not configured — using simulated metrics')
            availability_pct = 99.99
            p95_latency_ms = 45.2
            error_rate_pct = 0.01

        # Evaluate SLO
        availability_ok = availability_pct >= slo_availability_min
        p95_ok = p95_latency_ms <= slo_p95_max_ms
        error_ok = error_rate_pct <= slo_error_max_pct
        slo_passed = availability_ok and p95_ok and error_ok

        report = (
            f'Availability: {availability_pct:.4f}% (min: {slo_availability_min}%) {"PASS" if availability_ok else "FAIL"}\n'
            f'P95 Latency:  {p95_latency_ms:.1f}ms (max: {slo_p95_max_ms}ms) {"PASS" if p95_ok else "FAIL"}\n'
            f'Error Rate:   {error_rate_pct:.4f}% (max: {slo_error_max_pct}%) {"PASS" if error_ok else "FAIL"}\n'
            f'Overall SLO:  {"PASSED" if slo_passed else "FAILED"}'
        )

        print(report)

        # Write GitHub outputs
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f'slo_passed={"true" if slo_passed else "false"}\n')
            f.write(f'availability={availability_pct:.4f}\n')
            f.write(f'p95_latency_ms={p95_latency_ms:.1f}\n')
            f.write(f'error_rate_pct={error_rate_pct:.4f}\n')
            f.write(f'slo_report={report.replace(chr(10), " | ")}\n')

        sys.exit(0 if slo_passed else 1)
        PYEOF
      env:
        PROMETHEUS_URL: ${{ secrets.PROMETHEUS_URL }}
        NAMESPACE: ${{ env.STAGING_NS }}
        SLO_AVAILABILITY_MIN: ${{ env.SLO_AVAILABILITY_MIN }}
        SLO_P95_LATENCY_MAX_MS: ${{ env.SLO_P95_LATENCY_MAX_MS }}
        SLO_ERROR_RATE_MAX_PCT: ${{ env.SLO_ERROR_RATE_MAX_PCT }}
      continue-on-error: true

    - name: SLO gate summary
      run: |
        echo "## SLO Gate Results" >> "$GITHUB_STEP_SUMMARY"
        echo "" >> "$GITHUB_STEP_SUMMARY"
        echo "| Metric | Value | Threshold | Status |" >> "$GITHUB_STEP_SUMMARY"
        echo "|--------|-------|-----------|--------|" >> "$GITHUB_STEP_SUMMARY"
        echo "| Availability | ${{ steps.check.outputs.availability }}% | ≥ ${{ env.SLO_AVAILABILITY_MIN }}% | ${{ steps.check.outputs.slo_passed == 'true' && '✅' || '❌' }} |" >> "$GITHUB_STEP_SUMMARY"
        echo "| P95 Latency | ${{ steps.check.outputs.p95_latency_ms }}ms | ≤ ${{ env.SLO_P95_LATENCY_MAX_MS }}ms | ${{ steps.check.outputs.slo_passed == 'true' && '✅' || '❌' }} |" >> "$GITHUB_STEP_SUMMARY"
        echo "| Error Rate | ${{ steps.check.outputs.error_rate_pct }}% | ≤ ${{ env.SLO_ERROR_RATE_MAX_PCT }}% | ${{ steps.check.outputs.slo_passed == 'true' && '✅' || '❌' }} |" >> "$GITHUB_STEP_SUMMARY"
        echo "" >> "$GITHUB_STEP_SUMMARY"
        echo "**Decision: ${{ steps.check.outputs.slo_passed == 'true' && 'PROMOTE to production' || 'ROLLBACK staging' }}**" >> "$GITHUB_STEP_SUMMARY"

  # ══════════════════════════════════════════════════════════════════
  # Stage 4a: Auto-Promote to Production (SLO passed)
  # ══════════════════════════════════════════════════════════════════
  promote-production:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [preflight, slo-gate]
    if: needs.slo-gate.outputs.slo_passed == 'true'
    environment:
      name: production
      url: https://indestructibleeco.space
    steps:
    - name: Checkout
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
      with:
        token: ${{ secrets.AUTO_FIX_TOKEN }}

    - name: Authenticate to GCP
      uses: google-github-actions/auth@6fc4af4b145ae7821d527454aa9bd537d1f2dc5f  # v2.1.7
      with:
        workload_identity_provider: ${{ secrets.GCP_WORKLOAD_IDENTITY_PROVIDER }}
        service_account: ${{ secrets.GCP_SERVICE_ACCOUNT }}
      continue-on-error: true

    - name: Get GKE credentials
      uses: google-github-actions/get-gke-credentials@7a108e64ed8546fe38316b4086e91da13f4785e1  # v2.3.1
      with:
        cluster_name: ${{ env.GKE_CLUSTER }}
        location: ${{ env.GKE_REGION }}
      continue-on-error: true

    - name: Promote to production
      run: |
        IMAGE_TAG="${{ needs.preflight.outputs.image_tag }}"
        echo "SLO passed — promoting $IMAGE_TAG to production"

        # Update production kustomization
        if [ -f "k8s/base/kustomization.yaml" ]; then
          sed -i "s|newTag:.*|newTag: $IMAGE_TAG|g" k8s/base/kustomization.yaml
          echo "Updated k8s/base/kustomization.yaml with tag: $IMAGE_TAG"
        fi

        # Trigger ArgoCD production sync
        if command -v argocd &>/dev/null; then
          argocd app sync ${{ env.ARGOCD_APP_PRODUCTION }} \
            --server ${{ secrets.ARGOCD_SERVER }} \
            --auth-token ${{ secrets.ARGOCD_TOKEN }} \
            --timeout 600 && echo "Production ArgoCD sync triggered"
        else
          kubectl apply -k k8s/base/ --namespace=${{ env.PRODUCTION_NS }} 2>/dev/null || \
            echo "Production promotion simulated (kubectl not configured)"
        fi

    - name: Commit production image tag update
      run: |
        IMAGE_TAG="${{ needs.preflight.outputs.image_tag }}"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        git config user.name "github-actions[bot]"
        git add k8s/base/kustomization.yaml k8s/staging/kustomization.yaml 2>/dev/null || true
        if git diff --cached --quiet; then
          echo "No kustomization changes to commit"
        else
          COMMIT_MSG="chore(deploy): promote $IMAGE_TAG to production [skip ci]"
          git commit -m "$COMMIT_MSG"
          git push origin main
          echo "Committed production promotion"
        fi

    - name: Production promotion summary
      run: |
        echo "## Production Promotion Successful" >> "$GITHUB_STEP_SUMMARY"
        echo "" >> "$GITHUB_STEP_SUMMARY"
        echo "- **Image**: \`${{ needs.preflight.outputs.image_tag }}\`" >> "$GITHUB_STEP_SUMMARY"
        echo "- **Promoted at**: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> "$GITHUB_STEP_SUMMARY"
        echo "- **SLO**: All gates passed ✅" >> "$GITHUB_STEP_SUMMARY"

  # ══════════════════════════════════════════════════════════════════
  # Stage 4b: Auto-Rollback (SLO failed)
  # ══════════════════════════════════════════════════════════════════
  rollback-staging:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [preflight, slo-gate]
    if: needs.slo-gate.outputs.slo_passed == 'false'
    steps:
    - name: Checkout
      uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
      with:
        token: ${{ secrets.AUTO_FIX_TOKEN }}

    - name: Rollback staging via ArgoCD
      run: |
        IMAGE_TAG="${{ needs.preflight.outputs.image_tag }}"
        echo "SLO FAILED — rolling back staging deployment of $IMAGE_TAG"

        if command -v argocd &>/dev/null; then
          argocd app rollback ${{ env.ARGOCD_APP_STAGING }} \
            --server ${{ secrets.ARGOCD_SERVER }} \
            --auth-token ${{ secrets.ARGOCD_TOKEN }} && echo "ArgoCD rollback triggered"
        else
          kubectl rollout undo deployment/api-service \
            --namespace=${{ env.STAGING_NS }} 2>/dev/null || \
            echo "Rollback simulated (kubectl not configured)"
        fi

    - name: Open incident issue
      uses: actions/github-script@ed597411d8f924073f98dfc5c65a23a2325f34cd  # v8.0.0
      with:
        github-token: ${{ secrets.AUTO_FIX_TOKEN }}
        script: |
          const imageTag = '${{ needs.preflight.outputs.image_tag }}';
          const sloReport = '${{ needs.slo-gate.outputs.slo_report }}';
          const availability = '${{ needs.slo-gate.outputs.availability }}';
          const p95 = '${{ needs.slo-gate.outputs.p95_latency_ms }}';
          const errorRate = '${{ needs.slo-gate.outputs.error_rate_pct }}';

          const bold = (s) => `**${s}**`;
          const body = [
            '## SLO Gate Failure — Staging Rollback Triggered',
            '',
            `${bold('Image Tag')}: \`${imageTag}\``,
            `${bold('Timestamp')}: ${new Date().toISOString()}`,
            `${bold('Run')}: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}`,
            '',
            '### SLO Metrics at Time of Failure',
            '',
            '| Metric | Measured | Threshold | Status |',
            '|--------|----------|-----------|--------|',
            `| Availability | ${availability}% | >= ${{ env.SLO_AVAILABILITY_MIN }}% | FAIL |`,
            `| P95 Latency | ${p95}ms | <= ${{ env.SLO_P95_LATENCY_MAX_MS }}ms | FAIL |`,
            `| Error Rate | ${errorRate}% | <= ${{ env.SLO_ERROR_RATE_MAX_PCT }}% | FAIL |`,
            '',
            '### Actions Taken',
            `- Staging deployment of \`${imageTag}\` automatically rolled back via ArgoCD`,
            '- Production was NOT promoted',
            '',
            '### Next Steps',
            '- [ ] Investigate root cause in staging logs',
            '- [ ] Fix the underlying issue',
            '- [ ] Re-trigger canary deployment after fix',
          ].join('\n');

          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `[Incident] SLO Gate Failure — ${imageTag} rolled back`,
            body: body,
            labels: ['incident', 'slo-failure', 'automated', 'canary']
          });
          console.log('Incident issue created');

    - name: Rollback summary
      run: |
        echo "## Staging Rollback Triggered" >> "$GITHUB_STEP_SUMMARY"
        echo "" >> "$GITHUB_STEP_SUMMARY"
        echo "- **Image**: \`${{ needs.preflight.outputs.image_tag }}\` — **NOT promoted**" >> "$GITHUB_STEP_SUMMARY"
        echo "- **Reason**: SLO gate failed" >> "$GITHUB_STEP_SUMMARY"
        echo "- **Action**: Staging rolled back, incident issue opened" >> "$GITHUB_STEP_SUMMARY"
